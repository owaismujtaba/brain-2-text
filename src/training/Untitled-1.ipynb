{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 139M/139M [00:12<00:00, 11.5MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribed text: \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import whisper\n",
    "\n",
    "# Load Whisper model (you can use \"base\", \"small\", \"medium\", \"large\")\n",
    "model = whisper.load_model(\"base\")\n",
    "\n",
    "# Example: your numpy array (simulate 1-second audio at 16kHz)\n",
    "# Make sure it's float32 and in the range [-1, 1]\n",
    "# shape: (num_samples,)\n",
    "audio_np = np.random.randn(16000).astype(np.float32)\n",
    "audio_np /= np.max(np.abs(audio_np))  # normalize\n",
    "\n",
    "# Convert numpy array to torch tensor\n",
    "audio_tensor = torch.from_numpy(audio_np)\n",
    "\n",
    "# Whisper expects 1D tensor on CPU\n",
    "if len(audio_tensor.shape) > 1:\n",
    "    audio_tensor = audio_tensor.squeeze()\n",
    "\n",
    "# Transcribe\n",
    "result = model.transcribe(audio_tensor.numpy())\n",
    "print(\"Transcribed text:\", result[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/openai/whisper.git torch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import whisper\n",
    "from whisper.tokenizer import get_tokenizer\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 1. Define a simple upstream model\n",
    "# -------------------------------\n",
    "class UpstreamModel(nn.Module):\n",
    "    def __init__(self, input_dim=80, hidden_dim=512, output_dim=512):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # x: (batch, time, input_dim)\n",
    "        return self.net(x)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Load Whisper & freeze it\n",
    "# -------------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "whisper_model = whisper.load_model(\"base\", device=device)\n",
    "\n",
    "# Freeze all Whisper params\n",
    "for p in whisper_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "decoder = whisper_model.decoder  # keep decoder only\n",
    "tokenizer = get_tokenizer(multilingual=True)\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Combined model\n",
    "# -------------------------------\n",
    "class UpstreamToWhisper(nn.Module):\n",
    "    def __init__(self, upstream, decoder):\n",
    "        super().__init__()\n",
    "        self.upstream = upstream\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x, tokens):\n",
    "        # x -> upstream features\n",
    "        encoder_out = self.upstream(x)  # (batch, time, 512)\n",
    "        # Feed into Whisper decoder\n",
    "        return self.decoder(tokens, encoder_out)\n",
    "\n",
    "# Instantiate\n",
    "upstream = UpstreamModel().to(device)\n",
    "model = UpstreamToWhisper(upstream, decoder).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 4. Dummy training example\n",
    "# -------------------------------\n",
    "# Example fake input (batch=2, time=100, features=80)\n",
    "dummy_audio_feats = torch.randn(2, 100, 80).to(device)\n",
    "\n",
    "# Dummy target text\n",
    "target_texts = [\"My mother is from an italian village\", \"openai rocks\"]\n",
    "token_ids = [tokenizer.encode(t) for t in target_texts]\n",
    "\n",
    "# Pad token ids\n",
    "max_len = max(len(t) for t in token_ids)\n",
    "tokens = torch.full((len(token_ids), max_len), tokenizer.eot, dtype=torch.long, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
