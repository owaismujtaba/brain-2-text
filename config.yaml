experiment:
  name: brain-2-text
  description: Text decodinf from brain signals
  version: 1.0.0

logging:
  name: Training
  level: INFO
  format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
  log_file_path: 'logs/training.log'

run:
  mode: train  # Options: train, inference
  random_seed: 42

dataset:
  data_folder: '/home/owaismujtaba/work/brain-2-text/data'
  num_workers: 4
  pin_memory: True
  prefetch_factor: 1



training:
  epochs: 1000
  batch_size: 6
  learning_rate: 0.001
  checkpoints_save_interval: 50
  load_checkpoint: False
  output_dir: 'outputs/'
  checkpoints_dir: 'checkpoints/'
  checkpoint_file : 'checkpoints/best_model.pt'

model:
  input_dim: 512 
  hidden_dim: 512
  num_layers: 2
  num_classes: 41  # Vocabulary size
  dropout: 0.1




