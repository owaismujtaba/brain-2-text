experiment:
  name: brain-2-text
  description: Text decodinf from brain signals
  version: 1.0.0

logging:
  name: brain2text
  level: INFO
  format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
  log_file_path: 'logs/experiment.log'

run:
  mode: train  # Options: train, inference
  random_seed: 42

dataset:
  data_folder: '/home/owaismujtaba/work/brain-2-text/data'

training:
  must_include_days: 5
  epochs: 100
  batch_size: 4
  learning_rate: 0.0001
  weight_decay: 0.00001
  checkpoints_dir: 'checkpoints/'

model:
  input_dim: 512 
  hidden_dim: 512
  num_layers: 2
  num_classes: 41  # Vocabulary size
  dropout: 0.1




